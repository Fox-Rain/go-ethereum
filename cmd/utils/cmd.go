// Copyright 2014 The go-ethereum Authors
// This file is part of go-ethereum.
//
// go-ethereum is free software: you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.
//
// go-ethereum is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
// GNU General Public License for more details.
//
// You should have received a copy of the GNU General Public License
// along with go-ethereum. If not, see <http://www.gnu.org/licenses/>.

// Package utils contains internal helper functions for go-ethereum commands.

// TODO: gethì— í•„ìš”í•œ ë…¸ë“œì‹œì‘, ì„œë¹„ìŠ¤ë“±ë¡, ì„¤ì • ì ìš©ë“±ì˜ í•µì‹¬ ìœ í‹¸ í•¨ìˆ˜ë¥¼ ë‹´ê³  ìˆë‹¤.
// 		 ì¦‰, ëª…ë ¹ì–´ë“¤ì„ ì…ë ¥ì‹œ ì‹¤ì œ ë™ì‘ì„ ë§Œë“œëŠ” "ì—”ì§„í•¨ìˆ˜ë“¤"ì´ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì´ë‹¤.
// TODO: config.goê°€ ë…¸ë“œì˜ ì„¤ì •ê°’ì„ êµ¬ì„±í•˜ê³  ë…¸ë“œë¥¼ ìƒì„±í•œë‹¤ë©´ ê·¸ ì„¤ì •ê°’ì— ë”°ë¼ ë°”ê¾¸ëŠ” í•µì‹¬ ì—”ì§„ì—­í• ì˜ í•¨ìˆ˜ëŠ” cmd.goì— ì¡´ì¬

/*

	| í•­ëª©           | Era1 (`ExportHistory`)                       | ì¼ë°˜ export (`ExportChain`, `ExportAppendChain`) |
| ------------ | -------------------------------------------- | ---------------------------------------------- |
| ğŸ“ íŒŒì¼ í˜•ì‹     | Geth ë‚´ë¶€ìš© í¬ë§· (`.era1`)                        | RLP ìŠ¤íŠ¸ë¦¼ (`.rlp`, `.rlp.gz`)                    |
| ğŸ“¦ í¬í•¨ ë‚´ìš©     | **ë¸”ë¡ + receipts + ëˆ„ì  ë‚œì´ë„ + Merkle root**     | **ë¸”ë¡ë§Œ** (í—¤ë” + íŠ¸ëœì­ì…˜ + uncle)                    |
| ğŸ“ ì €ì¥ ë‹¨ìœ„     | `step` í¬ê¸°ë¡œ ë¸”ë¡ë“¤ì„ ë‚˜ëˆ  ì—¬ëŸ¬ íŒŒì¼ë¡œ ì €ì¥                 | ì „ì²´ ì²´ì¸ í˜¹ì€ ì§€ì • êµ¬ê°„ì„ í•˜ë‚˜ì˜ íŒŒì¼ë¡œ ì €ì¥                     |
| ğŸ§¾ ë¬´ê²°ì„± ì²´í¬    | ê° íŒŒì¼ë§ˆë‹¤ SHA256 ì²´í¬ì„¬ + Merkle root í¬í•¨           | ì—†ìŒ (ìˆ˜ë™ ê²€ì¦í•´ì•¼ í•¨)                                 |
| ğŸ§¬ êµ¬ì¡°í™”       | `era.Filename()`, `era.NewBuilder()` ë“±ìœ¼ë¡œ êµ¬ì¡°í™” | ë‹¨ìˆœí•œ RLP ë¦¬ìŠ¤íŠ¸                                    |
| ğŸ“¦ import ëŒ€ìƒ | `ImportHistory()`ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥                   | `ImportChain()`ì—ì„œ ì‚¬ìš© ê°€ëŠ¥                        |
| ğŸ” ë³´ì•ˆì„±/ë¬´ê²°ì„±   | ìƒëŒ€ì ìœ¼ë¡œ ë” ì•ˆì „í•˜ê³  ê²€ì¦ ê°€ëŠ¥                           | êµ¬ì¡°ê°€ ë‹¨ìˆœí•˜ê³ , ê²€ì¦ì€ ìˆ˜ë™ì´ê±°ë‚˜ insert ì‹œ ê²€ì¦ë¨               |



* History == ì œë„¤ì‹œìŠ¤ë¶€í„° íŠ¹ì • ì‹œì ê¹Œì§€ì˜ ì „ì²´ ì²´ì¸ ë°ì´í„° (ë¸”ë¡, ìƒíƒœ ë£¨íŠ¸, receipt, total difficulty ë“± í¬í•¨)




cmd.go

start node
ì‹œìŠ¤í…œ ë””ìŠ¤í¬ ìš©ëŸ‰ ë¶€ì¡± ê°ì‹œ
ì²´ì¸ì— ë°ì´í„° ë„£ê¸° / ë¹¼ê¸° ë“±...  + ì—¬ëŸ¬ í¬ë©§ rlp, era1 ..		** ëŒ€ë¶€ë¶„ gethì„ í†µí•´ í„°ë¯¸ë„ ì»¤ë§¨ë“œë¡œ ì‹¤í–‰ë˜ëŠ” í•¨ìˆ˜ë“¤ì´ë‹¤.


*/

package utils

import (
	"bufio"
	"bytes"
	"compress/gzip"
	"crypto/sha256"
	"errors"
	"fmt"
	"io"
	"math/big"
	"os"
	"os/signal"
	"path/filepath"
	"runtime"
	"strings"
	"syscall"
	"time"

	"github.com/ethereum/go-ethereum/common"
	"github.com/ethereum/go-ethereum/core"
	"github.com/ethereum/go-ethereum/core/rawdb"
	"github.com/ethereum/go-ethereum/core/state/snapshot"
	"github.com/ethereum/go-ethereum/core/types"
	"github.com/ethereum/go-ethereum/crypto"
	"github.com/ethereum/go-ethereum/eth/ethconfig"
	"github.com/ethereum/go-ethereum/ethdb"
	"github.com/ethereum/go-ethereum/internal/debug"
	"github.com/ethereum/go-ethereum/internal/era"
	"github.com/ethereum/go-ethereum/log"
	"github.com/ethereum/go-ethereum/node"
	"github.com/ethereum/go-ethereum/params"
	"github.com/ethereum/go-ethereum/rlp"
	"github.com/urfave/cli/v2"
)

const (
	importBatchSize = 2500	// * ë¸”ë¡ì´ë‚˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ë–„  2500ë‹¨ìœ„ë¡œ ë°°ì¹˜ ì²˜ë¦¬í•˜ê² ë‹¤ëŠ” ì„¤ì •ê°’
)

// ErrImportInterrupted is returned when the user interrupts the import process.
var ErrImportInterrupted = errors.New("interrupted")

// Fatalf formats a message to standard error and exits the program.
// The message is also printed to standard output if standard error
// is redirected to a different file.

// osì— ë”°ë¼ ì¶œë ¥ ëŒ€ìƒì„ ì •í•˜ê³  í¬íŒ…ëœ ì—ëŸ¬ ë©”ì„¸ì§€ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜
func Fatalf(format string, args ...interface{}) {
	w := io.MultiWriter(os.Stdout, os.Stderr)
	if runtime.GOOS == "windows" || runtime.GOOS == "openbsd" {
		// The SameFile check below doesn't work on Windows neither OpenBSD.
		// stdout is unlikely to get redirected though, so just print there.
		w = os.Stdout
	} else {
		outf, _ := os.Stdout.Stat()
		errf, _ := os.Stderr.Stat()
		if outf != nil && errf != nil && os.SameFile(outf, errf) {
			w = os.Stderr
		}
	}
	fmt.Fprintf(w, "Fatal: "+format+"\n", args...)
	os.Exit(1)
}



// TODO: main.goì˜ StartNodeì—ì„œ í˜¸ì¶œë˜ëŠ” í•¨ìˆ˜ë¡œ   ì„¤ì •ëœ ìŠ¤íƒì„ ì‹¤ì œë¡œ ì‹œì‘í•˜ê³  ì¸í„°ëŸ½íŠ¸ ì‹œê·¸ë„ ì²˜ë¦¬, ë””ìŠ¤í¬ ê°ì‹œ, ... ë“±ì„ ë‹´ë‹¹
func StartNode(ctx *cli.Context, stack *node.Node, isConsole bool) {

	// TODO: node.Node íƒ€ì…ì¸ stackì„ ì‹¤í–‰ => ë‚´ë¶€ì ìœ¼ë¡œ p2p, RPC, consensusë“± ëª¨ë“  ëª¨ë“ˆì„ ì‹œì‘í•œë‹¤.  **ë…¸ë“œ ë‚´ë¶€ì— ëª¨ë“ˆì´ ì—°ê²°ë˜ì–´ì„œ ë…¸ë“œë¥¼ í†µí•´ ë‚´ë¶€ì ìœ¼ë¡œ ì‹œì‘ëœë‹¤.
	if err := stack.Start(); err != nil {
		Fatalf("Error starting protocol stack: %v", err)
	}

	// TODO: ê³ ë£¨í‹´ì„ í†µí•´ ë°±ê·¸ë¼ìš´ë“œë¡œ ì¢…ë£Œ ì‹œê·¸ë„ ëŒ€ê¸°, ë””ìŠ¤í¬ ìƒíƒœ ê°ì‹œ, graceful shutdownì„ ì²˜ë¦¬í•œë‹¤.
	go func() {

		// OSì—ì„œ ì˜¤ëŠ” ì¢…ë£Œìš”ì²­ì„ ê°ì§€í•˜ëŠ” ì±„ë„ì„ ìƒì„± -> ì‹œê·¸ë„ ë°›ìœ¼ë©´ ê·¸ì— ë”°ë¼ ì¢…ë£Œë£¨í‹´ ìˆ˜í–‰	 ì´ ë•Œë¬¸ì— ì´ ê³ ë£¨í‹´ì´ ê³„ì† ìœ ì§€ëœë‹¤.
		sigc := make(chan os.Signal, 1)
		signal.Notify(sigc, syscall.SIGINT, syscall.SIGTERM)
		defer signal.Stop(sigc)
		
		// ìµœì†Œ ë””ìŠ¤í¬ ê³µê°„ ì„¤ì • optional
		minFreeDiskSpace := 2 * ethconfig.Defaults.TrieDirtyCache // Default 2 * 256Mb
		if ctx.IsSet(MinFreeDiskSpaceFlag.Name) {
			minFreeDiskSpace = ctx.Int(MinFreeDiskSpaceFlag.Name)
		} else if ctx.IsSet(CacheFlag.Name) || ctx.IsSet(CacheGCFlag.Name) {
			minFreeDiskSpace = 2 * ctx.Int(CacheFlag.Name) * ctx.Int(CacheGCFlag.Name) / 100
		}

		// ë””ìŠ¤í¬ ì—¬ìœ ê³µê°„ì´ ë¶€ì¡±í•´ì§€ë©´ ë¡œê·¸ ê²½ê³  ì¶œë ¥ìš© ê³ ë£¨í‹´ ì‹¤í–‰
		if minFreeDiskSpace > 0 {
			// ì•„ë˜ì˜ ê³ ë£¨í‹´ì´ ë‚´ë¶€ì ìœ¼ë¡œ ë°˜ë³µì ìœ¼ë¡œ ë””ìŠ¤í¬ ìƒíƒœë¥¼ ê°ì‹œí•˜ëŠ” ë£¨í”„ë¥¼ ëŒë¦°ë‹¤.
			go monitorFreeDiskSpace(sigc, stack.InstanceDir(), uint64(minFreeDiskSpace)*1024*1024)
		}

		// ë…¸ë“œ ì¢…ë£Œí• ë–„ ì‹¤í–‰ë˜ëŠ” ë£¨í‹´
		shutdown := func() {
			log.Info("Got interrupt, shutting down...")
			go stack.Close()
			for i := 10; i > 0; i-- {
				<-sigc
				if i > 1 {
					log.Warn("Already shutting down, interrupt more to panic.", "times", i-1)
				}
			}
			debug.Exit() // ensure trace and CPU profile data is flushed.
			debug.LoudPanic("boom")
		}

		if isConsole {
			// In JS console mode, SIGINT is ignored because it's handled by the console.
			// However, SIGTERM still shuts down the node.
			for {
				sig := <-sigc
				if sig == syscall.SIGTERM {
					shutdown()
					return
				}
			}
		} else {
			<-sigc
			shutdown()
		}
	}()
}



// ë””ìŠ¤í¬ ê²½ë¡œ pathì˜ ì‚¬ìš©ê³µê°„ì´ freeDiskSpaceCritical ë³´ë‹¤ ì‘ì•„ì§€ëŠ”ì§€ ì£¼ê¸°ì ìœ¼ë¡œ ê²€ì‚¬í•œë‹¤.
// ì´ í•¨ìˆ˜ëŠ” startNode í•¨ìˆ˜ì•ˆì—ì„œ ê³ ë£¨í‹´ìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ê³„ì† ì‹¤í–‰ëœë‹¤.
func monitorFreeDiskSpace(sigc chan os.Signal, path string, freeDiskSpaceCritical uint64) {
	if path == "" {
		return
	}
	for {
		freeSpace, err := getFreeDiskSpace(path)
		if err != nil {
			log.Warn("Failed to get free disk space", "path", path, "err", err)
			break
		}

		// ë”§í¬ ê³µê°„ì´ ë¶€ì¡±í•˜ë‹¤ë©´ ë¡œê·¸ ì¶œë ¥ + startnodeì˜ ê³ ë£¨í‹´ì— signalì„ ë³´ë‚´ê³  shutdowní˜¸ì¶œ -> geth ì¢…ë£Œ
		if freeSpace < freeDiskSpaceCritical {
			log.Error("Low disk space. Gracefully shutting down Geth to prevent database corruption.", "available", common.StorageSize(freeSpace), "path", path)
			sigc <- syscall.SIGTERM
			break
		} else if freeSpace < 2*freeDiskSpaceCritical {
			log.Warn("Disk space is running low. Geth will shutdown if disk space runs below critical level.", "available", common.StorageSize(freeSpace), "critical_level", common.StorageSize(freeDiskSpaceCritical), "path", path)
		}
		time.Sleep(30 * time.Second)
	}
}



// gethì—ì„œ ë¸”ë¡ì²´ì¸ ë°ì´í„°ë¥¼ RLP í¬ë§·ìœ¼ë¡œ ì €ì¥ëœ íŒŒì¼ì—ì„œ ì½ì–´ì™€ ì²´ì¸ì— ì‚½ì…í•˜ëŠ” ê¸°ëŠ¥ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜
// íŒŒì¼ì—ì„œ ì½ì€ ë¸”ë¡ë“¤ì„ ì§ì ‘ ë‚´ ë¡œì»¬ ì²´ì¸ì— ì‚½ì…í•˜ëŠ” ê¸°ëŠ¥ìœ¼ë¡œ (offline) p2pë¡œ ë°›ëŠ”ê²Œ ì•„ë‹Œ ì˜¤í”„ë¼ì¸ íŒŒì¼ì„ í†µí•´ ì²´ì¸ì— ì‚½ì´í•˜ëŠ” ê²½ìš°
func ImportChain(chain *core.BlockChain, fn string) error {
	// Watch for Ctrl-C while the import is running.
	// If a signal is received, the import will stop at the next batch.
	interrupt := make(chan os.Signal, 1)		//  OSì—ì„œ SIGNT or SIGTERMì´ ë“¤ì–´ì˜¤ë©´ ë°›ëŠ” ì±„ë„
	stop := make(chan struct{})					// ë‚´ë¶€ ê³ ë£¨í‹´ê°„ í†µì‹ ìš©
	signal.Notify(interrupt, syscall.SIGINT, syscall.SIGTERM)
	defer signal.Stop(interrupt)
	defer close(interrupt)

	// ê³ ë£¨í‹´ìœ¼ë¡œ ë°±ê·¸ë¼ìš´ë“œì—ì„œ interruptë¥¼ ê°ì‹œí•¨  	interruptì— ì‹œê·¸ë„ì´ ë“¤ì–´ì˜¤ë©´ stop ì±„ë„ì„ ë‹«ëŠ”ë‹¤.
	go func() {
		if _, ok := <-interrupt; ok {
			log.Info("Interrupted during import, stopping at next batch")
		}
		close(stop)
	}()
	// stop ì±„ë„ì´ ë‹«í˜”ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë©”ì„œë“œ
	checkInterrupt := func() bool {
		select {
		case <-stop:
			return true
		default:
			return false
		}
	}

	log.Info("Importing blockchain", "file", fn)

	// Open the file handle and potentially unwrap the gzip stream
	// fnê²½ë¡œì—ì„œ íŒŒì¼ì„ ì—´ê³   
	fh, err := os.Open(fn)
	if err != nil {
		return err
	}
	defer fh.Close()
	
	// gzipì´ë©´ ì••ì¶• í•´ì œ ì²˜ë¦¬
	var reader io.Reader = fh
	if strings.HasSuffix(fn, ".gz") {
		if reader, err = gzip.NewReader(reader); err != nil {
			return err
		}
	}
	
	// rlp.NewStreamìœ¼ë¡œ RLP ë””ì½”ë”© ìŠ¤íŠ¸ë¦¼ ìƒì„±		*ì§ë ¬í™”ëœ ë¸”ë¡ ë°ì´í„°ë¥¼ -> êµ¬ì¡°ì²´ë¡œ ë””ì½”ë”©í•˜ëŠ” ìŠ¤íŠ¸ë¦¼
	stream := rlp.NewStream(reader, 0)

	// Run actual the import. ë¸”ë¡ì„ ë°°ì¹˜ë‹¨ìœ„ë¡œ ì½ê³  ì²´ì¸ì— Import
	blocks := make(types.Blocks, importBatchSize)
	n := 0
	for batch := 0; ; batch++ {
		// Load a batch of RLP blocks.
		if checkInterrupt() {				// interrupt check
			return ErrImportInterrupted
		}

		// RLP block decoding
		i := 0
		for ; i < importBatchSize; i++ {
			var b types.Block
			if err := stream.Decode(&b); err == io.EOF {
				break
			} else if err != nil {
				return fmt.Errorf("at block %d: %v", n, err)
			}
			// don't import first block
			if b.NumberU64() == 0 {
				i--
				continue
			}
			blocks[i] = &b
			n++
		}
		if i == 0 {
			break
		}
		// Import the batch.
		if checkInterrupt() {
			return errors.New("interrupted")
		}

		// ì‹¤ì œë¡œ í•„ìš”í•œ ë¸”ë¡ë§Œ í•„í„°ë§
		missing := missingBlocks(chain, blocks[:i])
		if len(missing) == 0 {
			log.Info("Skipping batch as all blocks present", "batch", batch, "first", blocks[0].Hash(), "last", blocks[i-1].Hash())
			continue
		}

		// ì²´ì¸ì— ë¸”ë¡ ì‚½ì…		 ì´ë–„ ìœ íš¨ì„±ê²€ì‚¬ë„ í•˜ë©´ì„œ ë„£ëŠ”ë‹¤.
		if failindex, err := chain.InsertChain(missing); err != nil {
			var failnumber uint64
			if failindex > 0 && failindex < len(missing) {
				failnumber = missing[failindex].NumberU64()
			} else {
				failnumber = missing[0].NumberU64()
			}
			return fmt.Errorf("invalid block %d: %v", failnumber, err)
		}
	}
	return nil
}



// í…ìŠ¤íŠ¸ íŒŒì¼ì„ ì½ì–´ì„œ ì¤„ ë‹¨ìœ„ ë¬¸ìì—´ ë°°ì—´ë¡œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ util
func readList(filename string) ([]string, error) {
	b, err := os.ReadFile(filename)
	if err != nil {
		return nil, err
	}
	return strings.Split(string(b), "\n"), nil
}



// ImportHistory imports Era1 files containing historical block information,
// starting from genesis. The assumption is held that the provided chain
// segment in Era1 file should all be canonical and verified.

// Era1 í˜•ì‹ì˜ íˆìŠ¤í† ë¦¬ ë¸”ë¡ ë°ì´í„°íŒŒì¼ì„ ê°€ì ¸ì™€ ì²´ì¸ì— ì‚½ì…í•˜ëŠ” í•¨ìˆ˜
func ImportHistory(chain *core.BlockChain, dir string, network string) error {
	if chain.CurrentSnapBlock().Number.BitLen() != 0 {
		return errors.New("history import only supported when starting from genesis")
	}
	entries, err := era.ReadDir(dir, network)
	if err != nil {
		return fmt.Errorf("error reading %s: %w", dir, err)
	}
	checksums, err := readList(filepath.Join(dir, "checksums.txt"))
	if err != nil {
		return fmt.Errorf("unable to read checksums.txt: %w", err)
	}
	if len(checksums) != len(entries) {
		return fmt.Errorf("expected equal number of checksums and entries, have: %d checksums, %d entries", len(checksums), len(entries))
	}
	var (
		start    = time.Now()
		reported = time.Now()
		imported = 0
		h        = sha256.New()
		buf      = bytes.NewBuffer(nil)
	)
	for i, filename := range entries {
		err := func() error {
			f, err := os.Open(filepath.Join(dir, filename))
			if err != nil {
				return fmt.Errorf("unable to open era: %w", err)
			}
			defer f.Close()

			// Validate checksum.
			if _, err := io.Copy(h, f); err != nil {
				return fmt.Errorf("unable to recalculate checksum: %w", err)
			}
			if have, want := common.BytesToHash(h.Sum(buf.Bytes()[:])).Hex(), checksums[i]; have != want {
				return fmt.Errorf("checksum mismatch: have %s, want %s", have, want)
			}
			h.Reset()
			buf.Reset()

			// Import all block data from Era1.
			e, err := era.From(f)
			if err != nil {
				return fmt.Errorf("error opening era: %w", err)
			}
			it, err := era.NewIterator(e)
			if err != nil {
				return fmt.Errorf("error making era reader: %w", err)
			}
			for it.Next() {
				block, err := it.Block()
				if err != nil {
					return fmt.Errorf("error reading block %d: %w", it.Number(), err)
				}
				if block.Number().BitLen() == 0 {
					continue // skip genesis
				}
				receipts, err := it.Receipts()
				if err != nil {
					return fmt.Errorf("error reading receipts %d: %w", it.Number(), err)
				}
				encReceipts := types.EncodeBlockReceiptLists([]types.Receipts{receipts})
				if _, err := chain.InsertReceiptChain([]*types.Block{block}, encReceipts, 2^64-1); err != nil {
					return fmt.Errorf("error inserting body %d: %w", it.Number(), err)
				}
				imported += 1

				// Give the user some feedback that something is happening.
				if time.Since(reported) >= 8*time.Second {
					log.Info("Importing Era files", "head", it.Number(), "imported", imported, "elapsed", common.PrettyDuration(time.Since(start)))
					imported = 0
					reported = time.Now()
				}
			}
			return nil
		}()
		if err != nil {
			return err
		}
	}

	return nil
}



// ì£¼ì–´ì§„ ë¸”ë¡ ë¦¬ìŠ¤íŠ¸ ì¤‘ì—ì„œ ì•„ì§ ë¡œì»¬ì²´ì¸ì— ì—†ëŠ” ë¸”ë¡ë“¤ë§Œ ê³¨ë¼ë‚´ëŠ” í•¨ìˆ˜
func missingBlocks(chain *core.BlockChain, blocks []*types.Block) []*types.Block {
	head := chain.CurrentBlock()	// í˜„ì¬ ë¡œì»¬ì²´ì¸ì˜ head ë¸”ë¡ *í˜„ì¬ ì²´ì¸ì˜ ìƒíƒœë¥¼ ê°€ì ¸ì˜´
	for i, block := range blocks {
		// If we're behind the chain head, only check block, state is available at head
		// head ì¦‰, í˜„ì¬ ë¡œì»¬ì˜ ìµœì‹ ë¸”ë¡ ë³´ë‹¤  ì´ì „ë¸”ë¡ì˜ ê²½ìš°ëŠ” ì¡´ì¬í•˜ê¸°ë§Œí•˜ë©´ ëœë‹¤. ì—†ë‹¤ë©´ ë¦¬í„´
		if head.Number.Uint64() > block.NumberU64() {
			if !chain.HasBlock(block.Hash(), block.NumberU64()) {
				return blocks[i:]
			}
			continue
		}
		// If we're above the chain head, state availability is a must
		// í˜„ì¬ head ì´ìƒì´ë¼ë©´ ë¸”ë¡ + ìƒíƒœ ë‘˜ ë‹¤ ì¡´ì¬í•´ì•¼í•œë‹¤. ê·¸ ì‹œì ë¶€í„° ì „ë¶€ ë¦¬í„´í•¨
		if !chain.HasBlockAndState(block.Hash(), block.NumberU64()) {
			return blocks[i:]
		}
	}
	return nil
}



// ExportChain exports a blockchain into the specified file, truncating any data
// already present in the file.
// Gethì—ì„œ ë¸”ë¡ì²´ì¸ ë°ì´í„°ë¥¼ ë¡œì»¬ì— íŒŒì¼ë¡œ ì €ì¥í•  ë–„ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜
func ExportChain(blockchain *core.BlockChain, fn string) error {
	log.Info("Exporting blockchain", "file", fn)		// ì–´ë””ì— export ì €ì¥í•˜ëŠ”ì§€ ë¡œê·¸ë¥¼ ë‚¨ê¹€

	// Open the file handle and potentially wrap with a gzip stream
	// íŒŒì¼ì„ ì—°ë‹¤.
	fh, err := os.OpenFile(fn, os.O_CREATE|os.O_WRONLY|os.O_TRUNC, os.ModePerm)
	if err != nil {
		return err
	}
	defer fh.Close()

	// íŒŒì¼ì´ gzipì¸ì§€ ì•„ë‹Œì§€ì— ë”°ë¼ì„œ writerë¥¼ ê³¨ë¼ì„œ ì••ì¶•ì²˜ë¦¬
	var writer io.Writer = fh
	if strings.HasSuffix(fn, ".gz") {
		writer = gzip.NewWriter(writer)
		defer writer.(*gzip.Writer).Close()
	}
	// Iterate over the blocks and export them
	// export ì‹¤í–‰ (RLP ì§ë ¬í™”ì—¬ ìˆœì°¨ì ìœ¼ë¡œ ê¸°ë¡)
	if err := blockchain.Export(writer); err != nil {
		return err
	}
	log.Info("Exported blockchain", "file", fn)		// ì„±ê³µ ë¡œê·¸

	return nil
}



// ExportAppendChain exports a blockchain into the specified file, appending to
// the file if data already exists in it.
// Gethì—ì„œ ë¸”ë¡ì²´ì¸ ë°ì´í„°ë¥¼ ë¡œì»¬ì— íŒŒì¼ë¡œ ë¶€ë¶„ì ìœ¼ë¡œ ì €ì¥í• ë•Œ ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜
func ExportAppendChain(blockchain *core.BlockChain, fn string, first uint64, last uint64) error {
	log.Info("Exporting blockchain", "file", fn)

	// Open the file handle and potentially wrap with a gzip stream
	fh, err := os.OpenFile(fn, os.O_CREATE|os.O_APPEND|os.O_WRONLY, os.ModePerm)
	if err != nil {
		return err
	}
	defer fh.Close()

	var writer io.Writer = fh
	if strings.HasSuffix(fn, ".gz") {
		writer = gzip.NewWriter(writer)
		defer writer.(*gzip.Writer).Close()
	}
	// Iterate over the blocks and export them
	// ***  ë¶€ë¶„ì ìœ¼ë¡œ export
	if err := blockchain.ExportN(writer, first, last); err != nil {
		return err
	}
	log.Info("Exported blockchain to", "file", fn)
	return nil
}



// ExportHistory exports blockchain history into the specified directory,
// following the Era format.
// ë¸”ë¡ì²´ì¸ íˆìŠ¤í† ë¦¬ë¥¼ era1í¬ë©§ìœ¼ë¡œ êµ¬ê°„ë³„ë¡œ export ì €ì¥í•˜ëŠ” í•¨ìˆ˜
func ExportHistory(bc *core.BlockChain, dir string, first, last, step uint64) error {
	log.Info("Exporting blockchain history", "dir", dir)
	if head := bc.CurrentBlock().Number.Uint64(); head < last {
		log.Warn("Last block beyond head, setting last = head", "head", head, "last", last)
		last = head
	}
	network := "unknown"
	if name, ok := params.NetworkNames[bc.Config().ChainID.String()]; ok {
		network = name
	}
	if err := os.MkdirAll(dir, os.ModePerm); err != nil {
		return fmt.Errorf("error creating output directory: %w", err)
	}
	var (
		start     = time.Now()
		reported  = time.Now()
		h         = sha256.New()
		buf       = bytes.NewBuffer(nil)
		checksums []string
	)
	td := new(big.Int)
	for i := uint64(0); i < first; i++ {
		td.Add(td, bc.GetHeaderByNumber(i).Difficulty)
	}
	for i := first; i <= last; i += step {
		err := func() error {
			filename := filepath.Join(dir, era.Filename(network, int(i/step), common.Hash{}))
			f, err := os.Create(filename)
			if err != nil {
				return fmt.Errorf("could not create era file: %w", err)
			}
			defer f.Close()

			w := era.NewBuilder(f)
			for j := uint64(0); j < step && j <= last-i; j++ {
				var (
					n     = i + j
					block = bc.GetBlockByNumber(n)
				)
				if block == nil {
					return fmt.Errorf("export failed on #%d: not found", n)
				}
				receipts := bc.GetReceiptsByHash(block.Hash())
				if receipts == nil {
					return fmt.Errorf("export failed on #%d: receipts not found", n)
				}
				td.Add(td, block.Difficulty())
				if err := w.Add(block, receipts, new(big.Int).Set(td)); err != nil {
					return err
				}
			}
			root, err := w.Finalize()
			if err != nil {
				return fmt.Errorf("export failed to finalize %d: %w", step/i, err)
			}
			// Set correct filename with root.
			os.Rename(filename, filepath.Join(dir, era.Filename(network, int(i/step), root)))

			// Compute checksum of entire Era1.
			if _, err := f.Seek(0, io.SeekStart); err != nil {
				return err
			}
			if _, err := io.Copy(h, f); err != nil {
				return fmt.Errorf("unable to calculate checksum: %w", err)
			}
			checksums = append(checksums, common.BytesToHash(h.Sum(buf.Bytes()[:])).Hex())
			h.Reset()
			buf.Reset()
			return nil
		}()
		if err != nil {
			return err
		}
		if time.Since(reported) >= 8*time.Second {
			log.Info("Exporting blocks", "exported", i, "elapsed", common.PrettyDuration(time.Since(start)))
			reported = time.Now()
		}
	}

	os.WriteFile(filepath.Join(dir, "checksums.txt"), []byte(strings.Join(checksums, "\n")), os.ModePerm)

	log.Info("Exported blockchain to", "dir", dir)

	return nil
}



// ImportPreimages imports a batch of exported hash preimages into the database.
// It's a part of the deprecated functionality, should be removed in the future.
// Keccak í•´ì‹œì˜ preimage(ì›ë³¸ ë°ì´í„°)ë¥¼ ë””ì½”ë”©í•´ì„œ DBì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜		** keccak256 has -> preimage(ì›ì‹œ ë°ì´í„°) í˜•íƒœì˜ ë§¤í•‘ì„ ë””ìŠ¤í¬ì— ì €ì¥
// ì¦‰, ì›ë³¸ë°ì´í„°ë¥¼ í•´ì‹œí•˜ê³ , ê·¸ ê°’ê³¼ ì›ì‹œë°ì´í„°ë¼ë¦¬ ë§¤í•‘ì„ í†µí•´ì„œ   í•´ì‹œë§Œ ì €ì¥í•˜ëŠ” ê²½ìš°ë„ ë§¤í•‘ì„ í†µí•´ì„œ ì›ì‹œ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ìˆê²Œ í•¨.
func ImportPreimages(db ethdb.Database, fn string) error {
	log.Info("Importing preimages", "file", fn)

	// Open the file handle and potentially unwrap the gzip stream
	// íŒŒì¼ ì—´ê¸°  + gzip ì²˜ë¦¬
	fh, err := os.Open(fn)
	if err != nil {
		return err
	}
	defer fh.Close()

	var reader io.Reader = bufio.NewReader(fh)
	if strings.HasSuffix(fn, ".gz") {
		if reader, err = gzip.NewReader(reader); err != nil {
			return err
		}
	}

	// RLP ìŠ¤íŠ¸ë¦¼ ë””ì½”ë”© ì¤€ë¹„
	stream := rlp.NewStream(reader, 0)

	// Import the preimages in batches to prevent disk thrashing
	preimages := make(map[common.Hash][]byte)

	for {
		// Read the next entry and ensure it's not junk
		var blob []byte

		if err := stream.Decode(&blob); err != nil {
			if err == io.EOF {
				break
			}
			return err
		}
		// Accumulate the preimages and flush when enough ws gathered
		preimages[crypto.Keccak256Hash(blob)] = common.CopyBytes(blob)
		if len(preimages) > 1024 {
			rawdb.WritePreimages(db, preimages)
			preimages = make(map[common.Hash][]byte)
		}
	}
	// Flush the last batch preimage data
	if len(preimages) > 0 {
		rawdb.WritePreimages(db, preimages)
	}
	return nil
}



// ExportPreimages exports all known hash preimages into the specified file,
// truncating any data already present in the file.
// It's a part of the deprecated functionality, should be removed in the future.
// preimage DB -> íŒŒì¼ë¡œ ë‚´ë³´ë‚´ëŠ” í•¨ìˆ˜ export key-vale ì„ ì¼ë°˜ fileë¡œ ë‚´ë³´ëƒ„
func ExportPreimages(db ethdb.Database, fn string) error {
	log.Info("Exporting preimages", "file", fn)

	// Open the file handle and potentially wrap with a gzip stream
	fh, err := os.OpenFile(fn, os.O_CREATE|os.O_WRONLY|os.O_TRUNC, os.ModePerm)
	if err != nil {
		return err
	}
	defer fh.Close()

	var writer io.Writer = fh
	if strings.HasSuffix(fn, ".gz") {
		writer = gzip.NewWriter(writer)
		defer writer.(*gzip.Writer).Close()
	}
	// Iterate over the preimages and export them
	it := db.NewIterator([]byte("secure-key-"), nil)
	defer it.Release()

	for it.Next() {
		if err := rlp.Encode(writer, it.Value()); err != nil {
			return err
		}
	}
	log.Info("Exported preimages", "file", fn)
	return nil
}




// ExportSnapshotPreimages exports the preimages corresponding to the enumeration of
// the snapshot for a given root.
// TODO: snapshot.Tree ìŠ¤ëƒ…ìƒ· íŠ¸ë¦¬ì—ì„œ preimage ë°ì´í„°ë“¤ì„ ì¶”ì¶œí•˜ì—¬ì„œ RLPë¡œ ì¸ì½”ë”©í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
// 	     ìŠ¤ëƒ…ìƒ· íŠ¸ë¦¬ì—ëŠ” keyë“¤ì´ keccak256 í•´ì‹œê°’ë§Œ ì €ì¥ë˜ì–´ìˆê¸° ë•Œë¬¸ì—, ê·¸ì— í•´ë‹¹í•˜ëŠ” ì›ë˜ preimagesì„ DBì—ì„œ ì°¾ì•„ exportí•˜ëŠ” í•¨ìˆ˜
// *** accountì˜ hashë¥¼ í†µí•´ì„œ preimageë¥¼ ì½ì–´ì˜¤ëŠ” ê²ƒì„ gorutinìœ¼ë¡œ í•˜ì—¬ì„œ ì¡°íšŒ + ì €ì¥ì„ ë³‘ë ¬ì ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.
func ExportSnapshotPreimages(chaindb ethdb.Database, snaptree *snapshot.Tree, fn string, root common.Hash) error {
	log.Info("Exporting preimages", "file", fn)

	// íŒŒì¼ ì—´ê¸°
	fh, err := os.OpenFile(fn, os.O_CREATE|os.O_WRONLY|os.O_TRUNC, os.ModePerm)
	if err != nil {
		return err
	}
	defer fh.Close()

	// Enable gzip compressing if file name has gz suffix.
	// ì••ì¶•ì¤€ë¹„
	var writer io.Writer = fh
	if strings.HasSuffix(fn, ".gz") {
		gz := gzip.NewWriter(writer)
		defer gz.Close()
		writer = gz
	}
	buf := bufio.NewWriter(writer)
	defer buf.Flush()
	writer = buf


	type hashAndPreimageSize struct {
		Hash common.Hash
		Size int
	}
	hashCh := make(chan hashAndPreimageSize)

	var (
		start     = time.Now()
		logged    = time.Now()
		preimages int
	)

	// ê³ ë£¨í‹´ìœ¼ë¡œ ê° ê³„ì •ì˜ hashë¥¼ í†µí•´ì„œ preimage ìˆ˜ì§‘
	go func() {
		defer close(hashCh)
		accIt, err := snaptree.AccountIterator(root, common.Hash{})
		if err != nil {
			log.Error("Failed to create account iterator", "error", err)
			return
		}
		defer accIt.Release()

		for accIt.Next() {
			acc, err := types.FullAccount(accIt.Account())
			if err != nil {
				log.Error("Failed to get full account", "error", err)
				return
			}
			preimages += 1
			hashCh <- hashAndPreimageSize{Hash: accIt.Hash(), Size: common.AddressLength}

			if acc.Root != (common.Hash{}) && acc.Root != types.EmptyRootHash {
				stIt, err := snaptree.StorageIterator(root, accIt.Hash(), common.Hash{})
				if err != nil {
					log.Error("Failed to create storage iterator", "error", err)
					return
				}
				for stIt.Next() {
					preimages += 1
					hashCh <- hashAndPreimageSize{Hash: stIt.Hash(), Size: common.HashLength}

					if time.Since(logged) > time.Second*8 {
						logged = time.Now()
						log.Info("Exporting preimages", "count", preimages, "elapsed", common.PrettyDuration(time.Since(start)))
					}
				}
				stIt.Release()
			}
			if time.Since(logged) > time.Second*8 {
				logged = time.Now()
				log.Info("Exporting preimages", "count", preimages, "elapsed", common.PrettyDuration(time.Since(start)))
			}
		}
	}()

	// preiamge ì €ì¥
	for item := range hashCh {
		preimage := rawdb.ReadPreimage(chaindb, item.Hash)
		if len(preimage) == 0 {
			return fmt.Errorf("missing preimage for %v", item.Hash)
		}
		if len(preimage) != item.Size {
			return fmt.Errorf("invalid preimage size, have %d", len(preimage))
		}
		rlpenc, err := rlp.EncodeToBytes(preimage)
		if err != nil {
			return fmt.Errorf("error encoding preimage: %w", err)
		}
		if _, err := writer.Write(rlpenc); err != nil {
			return fmt.Errorf("failed to write preimage: %w", err)
		}
	}
	log.Info("Exported preimages", "count", preimages, "elapsed", common.PrettyDuration(time.Since(start)), "file", fn)
	return nil
}






// exportHeader is used in the export/import flow. When we do an export,
// the first element we output is the exportHeader.
// Whenever a backwards-incompatible change is made, the Version header
// should be bumped.
// If the importer sees a higher version, it should reject the import.


/*
	exportHeader : exportëœ íŒŒì¼ì˜ ë©”íƒ€ì •ë³´ ë¸”ë¡ìœ¼ë¡œ íŒŒì¼ ë§¨ ì•ì— RLP ì¸ì½”ë”©ì´ ë˜ì–´ ìˆë‹¤.
*/
type exportHeader struct {
	Magic    string // Always set to 'gethdbdump' for disambiguation   í¬ë§· ì‹ë³„ì
	Version  uint64 // í¬ë§·ë²„ì „
	Kind     string // ë°ì´í„° ì¢…ë¥˜	preimage, snapshot
	UnixTime uint64 // export íƒ€ì„
}

const exportMagic = "gethdbdump"
// exportëœ ë°ì´í„°ì—ì„œ ê° í‚¤-ê°’ ìŒì„ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í• ì§€ ëª…ì‹œí•˜ëŠ” ê²ƒ    0 : DBì— ì¶”ê°€		1 : DBì—ì„œ ì‚­ì œ
const (
	OpBatchAdd = 0
	OpBatchDel = 1
)




// ImportLDBData imports a batch of snapshot data into the database
// TODO: exportëœ preimageë‚˜ snapshot ë°ì´í„°ë¥¼ ë””ìŠ¤í¬ì—ì„œ ì½ì–´ì„œ LevelDBì— ë‹¤ì‹œ ì‚½ì…í•˜ëŠ” import í•¨ìˆ˜
// 	     ExportSnapShotPreimages()ì˜ ë°˜ëŒ€ì‘ì—…ì´ë‹¤.
func ImportLDBData(db ethdb.Database, f string, startIndex int64, interrupt chan struct{}) error {
	log.Info("Importing leveldb data", "file", f)

	// Open the file handle and potentially unwrap the gzip stream
	fh, err := os.Open(f)
	if err != nil {
		return err
	}
	defer fh.Close()

	var reader io.Reader = bufio.NewReader(fh)
	if strings.HasSuffix(f, ".gz") {
		if reader, err = gzip.NewReader(reader); err != nil {
			return err
		}
	}
	stream := rlp.NewStream(reader, 0)

	// Read the header
	var header exportHeader
	if err := stream.Decode(&header); err != nil {
		return fmt.Errorf("could not decode header: %v", err)
	}
	if header.Magic != exportMagic {
		return errors.New("incompatible data, wrong magic")
	}
	if header.Version != 0 {
		return fmt.Errorf("incompatible version %d, (support only 0)", header.Version)
	}
	log.Info("Importing data", "file", f, "type", header.Kind, "data age",
		common.PrettyDuration(time.Since(time.Unix(int64(header.UnixTime), 0))))

	// Import the snapshot in batches to prevent disk thrashing
	var (
		count  int64
		start  = time.Now()
		logged = time.Now()
		batch  = db.NewBatch()
	)
	for {
		// Read the next entry
		var (
			op       byte
			key, val []byte
		)
		if err := stream.Decode(&op); err != nil {
			if err == io.EOF {
				break
			}
			return err
		}
		if err := stream.Decode(&key); err != nil {
			return err
		}
		if err := stream.Decode(&val); err != nil {
			return err
		}
		if count < startIndex {
			count++
			continue
		}
		switch op {
		case OpBatchDel:
			batch.Delete(key)
		case OpBatchAdd:
			batch.Put(key, val)
		default:
			return fmt.Errorf("unknown op %d", op)
		}
		if batch.ValueSize() > ethdb.IdealBatchSize {
			if err := batch.Write(); err != nil {
				return err
			}
			batch.Reset()
		}
		// Check interruption emitted by ctrl+c
		if count%1000 == 0 {
			select {
			case <-interrupt:
				if err := batch.Write(); err != nil {
					return err
				}
				log.Info("External data import interrupted", "file", f, "count", count, "elapsed", common.PrettyDuration(time.Since(start)))
				return nil
			default:
			}
		}
		if count%1000 == 0 && time.Since(logged) > 8*time.Second {
			log.Info("Importing external data", "file", f, "count", count, "elapsed", common.PrettyDuration(time.Since(start)))
			logged = time.Now()
		}
		count += 1
	}
	// Flush the last batch snapshot data
	if batch.ValueSize() > 0 {
		if err := batch.Write(); err != nil {
			return err
		}
	}
	log.Info("Imported chain data", "file", f, "count", count,
		"elapsed", common.PrettyDuration(time.Since(start)))
	return nil
}




// ChainDataIterator is an interface wraps all necessary functions to iterate
// the exporting chain data.
type ChainDataIterator interface {
	// Next returns the key-value pair for next exporting entry in the iterator.
	// When the end is reached, it will return (0, nil, nil, false).
	Next() (byte, []byte, []byte, bool)

	// Release releases associated resources. Release should always succeed and can
	// be called multiple times without causing error.
	Release()
}

// ExportChaindata exports the given data type (truncating any data already present)
// in the file. If the suffix is 'gz', gzip compression is used.
// ì´ë”ë¦¬ì›€ ë°ì´í„° (preimage, shapshot)ë“±ì„ .ldb íŒŒì¼ë¡œ ë³´ë‚´ëŠ” ë²”ìš©ì ì¸ export í•¨ìˆ˜ì´ë‹¤.
// ì–´ë–¤ ì¢…ë¥˜ì˜ ì²´ì¸ë°ì´í„°ë„ exportí•  ìˆ˜ ìˆê²Œ ë§Œë“  ì¶”ìƒí™” ëœ ë²„ì „
func ExportChaindata(fn string, kind string, iter ChainDataIterator, interrupt chan struct{}) error {
	log.Info("Exporting chain data", "file", fn, "kind", kind)
	defer iter.Release()

	// Open the file handle and potentially wrap with a gzip stream
	fh, err := os.OpenFile(fn, os.O_CREATE|os.O_WRONLY|os.O_TRUNC, os.ModePerm)
	if err != nil {
		return err
	}
	defer fh.Close()

	var writer io.Writer = fh
	if strings.HasSuffix(fn, ".gz") {
		writer = gzip.NewWriter(writer)
		defer writer.(*gzip.Writer).Close()
	}
	// Write the header
	if err := rlp.Encode(writer, &exportHeader{
		Magic:    exportMagic,
		Version:  0,
		Kind:     kind,
		UnixTime: uint64(time.Now().Unix()),
	}); err != nil {
		return err
	}
	// Extract data from source iterator and dump them out to file
	var (
		count  int64
		start  = time.Now()
		logged = time.Now()
	)
	for {
		op, key, val, ok := iter.Next()
		if !ok {
			break
		}
		if err := rlp.Encode(writer, op); err != nil {
			return err
		}
		if err := rlp.Encode(writer, key); err != nil {
			return err
		}
		if err := rlp.Encode(writer, val); err != nil {
			return err
		}
		if count%1000 == 0 {
			// Check interruption emitted by ctrl+c
			select {
			case <-interrupt:
				log.Info("Chain data exporting interrupted", "file", fn,
					"kind", kind, "count", count, "elapsed", common.PrettyDuration(time.Since(start)))
				return nil
			default:
			}
			if time.Since(logged) > 8*time.Second {
				log.Info("Exporting chain data", "file", fn, "kind", kind,
					"count", count, "elapsed", common.PrettyDuration(time.Since(start)))
				logged = time.Now()
			}
		}
		count++
	}
	log.Info("Exported chain data", "file", fn, "kind", kind, "count", count,
		"elapsed", common.PrettyDuration(time.Since(start)))
	return nil
}
